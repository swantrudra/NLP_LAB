{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uKTR5UNuC_G",
        "outputId": "c182b710-e956-4e5b-8aa6-9f09e09ea581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is a leading platform for building Python programs to work with human language. Let's test tokenization, stemming, and lemmatization!\"\n"
      ],
      "metadata": {
        "id": "wqF98BBQuZr-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZi1INCzudew",
        "outputId": "7d84f226-225e-4c53-8db0-050e4601e267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language.', \"Let's\", 'test', 'tokenization,', 'stemming,', 'and', 'lemmatization!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(wordpunct_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew2D4iaTupXJ",
        "outputId": "1c4a38e4-91dc-4024-bbb7-9f6b2fbf1c12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', '.', 'Let', \"'\", 's', 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbt = TreebankWordTokenizer()\n",
        "print(tbt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efuPxxHXuzz2",
        "outputId": "a556da54-89bd-43bb-fe7f-048aeb80365c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language.', 'Let', \"'s\", 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet = \"NLTK is awesome!!! #NLP @OpenAI\"\n",
        "tt = TweetTokenizer()\n",
        "print(tt.tokenize(tweet))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVY2TDHVu-us",
        "outputId": "688bb74e-2a3d-4390-83c8-82885e8370ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'awesome', '!', '!', '!', '#NLP', '@OpenAI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('machine', 'learning'), ('natural', 'language', 'processing')])\n",
        "sentence = \"I love machine learning and natural language processing\"\n",
        "print(mwe.tokenize(sentence.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud-zejtnvHRB",
        "outputId": "5ba31774-fe75-43be-90fb-6c49d8b605d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'machine_learning', 'and', 'natural_language_processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\", \"fairly\"]\n",
        "print([ps.stem(word) for word in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy9B1peQvNDK",
        "outputId": "08fe5ff4-a297-45a4-e871-f275809f61b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print([ss.stem(word) for word in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLiQ6RzxvSu5",
        "outputId": "d8ab2489-bd58-4658-9fbb-cbea90fd0369"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'easili', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"cars\", pos=\"n\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpvBw67GvV5h",
        "outputId": "5055a075-434d-4a93-f36b-92a086314e9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n",
            "car\n"
          ]
        }
      ]
    }
  ]
}